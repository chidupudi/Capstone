# TrainForge Demo Training Job
# Shows difference between traditional vs distributed ML training

apiVersion: batch/v1
kind: Job
metadata:
  name: trainforge-traditional-training
  labels:
    app: trainforge-demo
    type: traditional
spec:
  template:
    metadata:
      labels:
        app: trainforge-demo
        type: traditional
    spec:
      restartPolicy: Never
      containers:
      - name: traditional-trainer
        image: python:3.9-slim
        imagePullPolicy: IfNotPresent
        
        resources:
          requests:
            cpu: "2.0"      # Single container gets 2 CPUs
            memory: "2Gi"   # 2GB memory
          limits:
            cpu: "2.0"
            memory: "2Gi"
        
        env:
        - name: TRAINING_MODE
          value: "traditional"
        - name: PYTHONUNBUFFERED
          value: "1"
        
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install numpy psutil scikit-learn
          python -c "
          import time
          import numpy as np
          import psutil
          from datetime import datetime
          from sklearn.datasets import make_classification
          from sklearn.ensemble import RandomForestClassifier
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import accuracy_score
          
          print('=== TRADITIONAL ML TRAINING (Single Process) ===')
          print(f'Started at: {datetime.now()}')
          print(f'CPU cores available: {psutil.cpu_count()}')
          print(f'Memory: {psutil.virtual_memory().total / (1024**3):.1f}GB')
          
          start_time = time.time()
          
          # Generate larger dataset for demonstration
          print('Generating dataset...')
          X, y = make_classification(
              n_samples=50000,      # 50k samples
              n_features=100,       # 100 features
              n_informative=50,     # 50 informative features
              n_redundant=20,       # 20 redundant features
              n_classes=10,         # 10 classes
              random_state=42
          )
          
          print(f'Dataset: {X.shape[0]} samples, {X.shape[1]} features, {len(set(y))} classes')
          
          # Split data
          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )
          
          # Traditional training (single process)
          print('\\nStarting traditional training...')
          
          # Train multiple models sequentially
          models = []
          for i in range(4):  # 4 models trained sequentially
              print(f'Training model {i+1}/4...')
              
              # Monitor resources
              cpu_before = psutil.cpu_percent(interval=1)
              memory_before = psutil.virtual_memory().percent
              
              # Create and train model
              model = RandomForestClassifier(
                  n_estimators=100,
                  max_depth=10,
                  random_state=42 + i,
                  n_jobs=1  # Single threaded
              )
              
              model.fit(X_train, y_train)
              predictions = model.predict(X_test)
              accuracy = accuracy_score(y_test, predictions)
              
              models.append({
                  'model_id': i,
                  'accuracy': accuracy,
                  'training_time': time.time() - start_time
              })
              
              # Monitor resources after
              cpu_after = psutil.cpu_percent(interval=1)
              memory_after = psutil.virtual_memory().percent
              
              print(f'  Model {i+1} - Accuracy: {accuracy:.3f}')
              print(f'  CPU: {cpu_before:.1f}% -> {cpu_after:.1f}%')
              print(f'  Memory: {memory_before:.1f}% -> {memory_after:.1f}%')
              print(f'  Elapsed time: {time.time() - start_time:.1f}s')
          
          total_time = time.time() - start_time
          avg_accuracy = np.mean([m['accuracy'] for m in models])
          
          print('\\n=== TRADITIONAL TRAINING RESULTS ===')
          print(f'Total training time: {total_time:.2f} seconds')
          print(f'Average accuracy: {avg_accuracy:.3f}')
          print(f'Models trained: {len(models)}')
          print(f'Training mode: Sequential (one model at a time)')
          print(f'Resource usage: Single process, limited CPU utilization')
          print('=====================================')
          "

---
apiVersion: batch/v1
kind: Job
metadata:
  name: trainforge-distributed-training
  labels:
    app: trainforge-demo
    type: distributed
spec:
  parallelism: 4  # 4 parallel workers
  completions: 4  # Need all 4 to complete
  template:
    metadata:
      labels:
        app: trainforge-demo
        type: distributed
    spec:
      restartPolicy: Never
      containers:
      - name: distributed-trainer
        image: python:3.9-slim
        imagePullPolicy: IfNotPresent
        
        resources:
          requests:
            cpu: "0.5"      # Each worker gets 0.5 CPUs
            memory: "512Mi" # 512MB per worker
          limits:
            cpu: "0.5"
            memory: "512Mi"
        
        env:
        - name: TRAINING_MODE
          value: "distributed"
        - name: WORKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PYTHONUNBUFFERED
          value: "1"
        
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install numpy psutil scikit-learn
          python -c "
          import time
          import numpy as np
          import psutil
          import os
          from datetime import datetime
          from sklearn.datasets import make_classification
          from sklearn.ensemble import RandomForestClassifier
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import accuracy_score
          
          worker_id = os.environ.get('WORKER_ID', 'unknown')
          
          print(f'=== DISTRIBUTED ML TRAINING - Worker {worker_id} ===')
          print(f'Started at: {datetime.now()}')
          print(f'CPU cores available: {psutil.cpu_count()}')
          print(f'Memory: {psutil.virtual_memory().total / (1024**3):.1f}GB')
          
          start_time = time.time()
          
          # Generate same dataset (in practice, this would be coordinated)
          print(f'Worker {worker_id}: Generating dataset...')
          X, y = make_classification(
              n_samples=50000,
              n_features=100,
              n_informative=50,
              n_redundant=20,
              n_classes=10,
              random_state=42  # Same seed for consistent comparison
          )
          
          # Split data
          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )
          
          # Each worker trains one model (distributed training)
          print(f'Worker {worker_id}: Starting distributed training...')
          
          # Monitor resources
          cpu_before = psutil.cpu_percent(interval=1)
          memory_before = psutil.virtual_memory().percent
          
          # Each worker gets different hyperparameters
          worker_configs = [
              {'n_estimators': 100, 'max_depth': 8},
              {'n_estimators': 150, 'max_depth': 10},
              {'n_estimators': 200, 'max_depth': 12},
              {'n_estimators': 250, 'max_depth': 15}
          ]
          
          # Use worker name hash to determine config
          config_idx = hash(worker_id) % len(worker_configs)
          config = worker_configs[config_idx]
          
          print(f'Worker {worker_id}: Using config {config}')
          
          # Train model
          model = RandomForestClassifier(
              n_estimators=config['n_estimators'],
              max_depth=config['max_depth'],
              random_state=42,
              n_jobs=-1  # Use all available cores in container
          )
          
          model.fit(X_train, y_train)
          predictions = model.predict(X_test)
          accuracy = accuracy_score(y_test, predictions)
          
          # Monitor resources after
          cpu_after = psutil.cpu_percent(interval=1)
          memory_after = psutil.virtual_memory().percent
          
          training_time = time.time() - start_time
          
          print(f'\\n=== WORKER {worker_id} RESULTS ===')
          print(f'Training time: {training_time:.2f} seconds')
          print(f'Accuracy: {accuracy:.3f}')
          print(f'Model config: {config}')
          print(f'CPU usage: {cpu_before:.1f}% -> {cpu_after:.1f}%')
          print(f'Memory usage: {memory_before:.1f}% -> {memory_after:.1f}%')
          print(f'Training mode: Distributed (parallel workers)')
          print('================================')
          
          # Simulate saving results (in practice, would coordinate with other workers)
          print(f'Worker {worker_id}: Training completed successfully!')
          "

---
# Service to monitor demo jobs
apiVersion: v1
kind: Service
metadata:
  name: trainforge-demo-monitor
  labels:
    app: trainforge-demo
spec:
  selector:
    app: trainforge-demo
  ports:
  - port: 8080
    name: monitor
  type: ClusterIP
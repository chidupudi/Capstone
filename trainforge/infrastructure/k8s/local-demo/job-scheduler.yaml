# TrainForge Job Scheduler for K8s
# Coordinates distributed training jobs across worker pods

apiVersion: apps/v1
kind: Deployment
metadata:
  name: trainforge-scheduler
  namespace: default
  labels:
    app: trainforge-scheduler
    component: control-plane
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trainforge-scheduler
  template:
    metadata:
      labels:
        app: trainforge-scheduler
        component: control-plane
    spec:
      serviceAccountName: trainforge-scheduler-sa
      containers:
      - name: scheduler
        image: python:3.9-slim
        imagePullPolicy: IfNotPresent
        
        resources:
          requests:
            cpu: "0.2"
            memory: "256Mi"
          limits:
            cpu: "0.5"
            memory: "512Mi"
        
        env:
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: SCHEDULER_MODE
          value: "distributed-cpu"
        
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install kubernetes requests psutil
          python -c "
          import time
          import json
          import os
          from datetime import datetime
          from kubernetes import client, config
          import requests
          
          print('TrainForge K8s Scheduler started')
          
          # Load K8s config
          try:
              config.load_incluster_config()
              print('Loaded in-cluster K8s config')
          except:
              config.load_kube_config()
              print('Loaded local K8s config')
          
          v1 = client.CoreV1Api()
          apps_v1 = client.AppsV1Api()
          
          def monitor_workers():
              '''Monitor worker pod status and performance'''
              try:
                  pods = v1.list_namespaced_pod(
                      namespace='default',
                      label_selector='app=trainforge-worker'
                  )
                  
                  worker_status = []
                  for pod in pods.items:
                      status = {
                          'name': pod.metadata.name,
                          'phase': pod.status.phase,
                          'node': pod.spec.node_name,
                          'cpu_request': pod.spec.containers[0].resources.requests.get('cpu', 'unknown'),
                          'memory_request': pod.spec.containers[0].resources.requests.get('memory', 'unknown'),
                          'started_at': pod.status.start_time.isoformat() if pod.status.start_time else None
                      }
                      worker_status.append(status)
                  
                  return worker_status
              except Exception as e:
                  print(f'Error monitoring workers: {e}')
                  return []
          
          def schedule_training_job(job_config):
              '''Schedule a new distributed training job'''
              print(f'Scheduling training job: {job_config.get(\"job_id\", \"unknown\")}')
              
              # Create job-specific ConfigMap
              job_config_map = client.V1ConfigMap(
                  metadata=client.V1ObjectMeta(
                      name=f'trainforge-job-{job_config[\"job_id\"][:8]}',
                      labels={'job-id': job_config['job_id'][:8]}
                  ),
                  data={
                      'job.json': json.dumps(job_config),
                      'status': 'scheduled'
                  }
              )
              
              try:
                  v1.create_namespaced_config_map(
                      namespace='default',
                      body=job_config_map
                  )
                  print(f'Created job ConfigMap for {job_config[\"job_id\"]}')
              except Exception as e:
                  print(f'Error creating job ConfigMap: {e}')
          
          # Main scheduler loop
          job_counter = 0
          while True:
              try:
                  current_time = datetime.now()
                  print(f'Scheduler heartbeat: {current_time}')
                  
                  # Monitor worker status
                  workers = monitor_workers()
                  running_workers = [w for w in workers if w['phase'] == 'Running']
                  
                  print(f'Active workers: {len(running_workers)}/{len(workers)}')
                  
                  for worker in running_workers:
                      print(f'  Worker {worker[\"name\"]}: {worker[\"phase\"]} on {worker[\"node\"]}')
                  
                  # Simulate job scheduling every 2 minutes
                  job_counter += 1
                  if job_counter % 24 == 0:  # Every 2 minutes (24 * 5s intervals)
                      mock_job = {
                          'job_id': f'job-{int(time.time())}',
                          'project_name': 'distributed-ml-demo',
                          'workers_required': len(running_workers),
                          'estimated_duration': 300,  # 5 minutes
                          'created_at': current_time.isoformat()
                      }
                      schedule_training_job(mock_job)
                  
                  # Report cluster status
                  if job_counter % 12 == 0:  # Every minute
                      cluster_status = {
                          'timestamp': current_time.isoformat(),
                          'total_workers': len(workers),
                          'running_workers': len(running_workers),
                          'scheduler_uptime': job_counter * 5,  # seconds
                          'jobs_scheduled': job_counter // 24
                      }
                      print(f'Cluster Status: {json.dumps(cluster_status, indent=2)}')
                  
                  time.sleep(5)  # Check every 5 seconds
                  
              except Exception as e:
                  print(f'Scheduler error: {e}')
                  time.sleep(10)
          "
        
        ports:
        - containerPort: 8080
          name: api
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
          failureThreshold: 3

---
# Service Account for scheduler to access K8s API
apiVersion: v1
kind: ServiceAccount
metadata:
  name: trainforge-scheduler-sa
  namespace: default

---
# ClusterRole for scheduler permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: trainforge-scheduler-role
rules:
- apiGroups: [""]
  resources: ["pods", "configmaps", "services"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

---
# ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: trainforge-scheduler-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: trainforge-scheduler-role
subjects:
- kind: ServiceAccount
  name: trainforge-scheduler-sa
  namespace: default

---
# Service for scheduler
apiVersion: v1
kind: Service
metadata:
  name: trainforge-scheduler-service
  labels:
    app: trainforge-scheduler
spec:
  selector:
    app: trainforge-scheduler
  ports:
  - port: 8080
    targetPort: 8080
    name: api
  type: ClusterIP
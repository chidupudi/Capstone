# TrainForge Configuration for Distributed CPU Demo
# Enhanced configuration with distributed processing settings

project:
  name: "distributed-cpu-demo"
  description: "Demonstration of distributed CPU processing with comprehensive statistics"
  version: "2.0"

# Training configuration
training:
  script: "train.py"
  type: "distributed"
  framework: "cpu-intensive"
  
  # Resource requirements
  resources:
    cpu:
      cores: 8
      memory_per_core_gb: 0.5
      affinity_enabled: true
      numa_awareness: true
    
    memory:
      total_gb: 4
      swap_allowed: false
    
    # No GPU requirement for this demo
    gpu:
      count: 0

# Distributed processing configuration
distributed:
  mode: "hybrid"  # multiprocessing, threading, hybrid, distributed
  max_workers: 8
  load_balancing: true
  fault_tolerance: true
  
  # Task configuration
  tasks:
    retry_count: 3
    timeout_seconds: 300
    priority_scheduling: true
    dependency_resolution: true
  
  # Monitoring configuration
  monitoring:
    enabled: true
    interval_seconds: 2
    metrics_retention: 1000
    performance_baseline: true

# Environment configuration
environment:
  base_image: "python:3.9-slim"
  
  requirements:
    - "numpy>=1.21.0"
    - "psutil>=5.8.0"
    - "matplotlib>=3.5.0"
  
  variables:
    PYTHONUNBUFFERED: "1"
    OMP_NUM_THREADS: "1"  # Prevent numpy from using multiple threads
    MKL_NUM_THREADS: "1"  # Intel MKL threading control
    NUMEXPR_NUM_THREADS: "1"  # NumExpr threading control

# Logging and output configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"
  
  outputs:
    console: true
    file: "training.log"
    statistics: "training_statistics.json"
    performance_graphs: true

# Experiment tracking
tracking:
  enabled: true
  
  metrics:
    - name: "cpu_utilization"
      type: "gauge"
      description: "CPU utilization percentage"
    
    - name: "memory_utilization"
      type: "gauge"
      description: "Memory utilization percentage"
    
    - name: "task_throughput"
      type: "rate"
      description: "Tasks processed per second"
    
    - name: "efficiency_score"
      type: "gauge"
      description: "Overall system efficiency score"
    
    - name: "worker_performance"
      type: "histogram"
      description: "Individual worker performance metrics"

# Advanced settings
advanced:
  # CPU optimization
  cpu_optimization:
    process_affinity: true
    numa_policy: "preferred"
    scheduler_policy: "normal"
    nice_level: 0
  
  # Memory optimization
  memory_optimization:
    huge_pages: false
    memory_mapping: "auto"
    garbage_collection: "adaptive"
  
  # Performance tuning
  performance:
    batch_processing: true
    pipeline_depth: 4
    prefetch_enabled: true
    cache_optimization: true

# Validation and testing
validation:
  enabled: true
  
  performance_tests:
    - name: "baseline_performance"
      description: "Measure baseline system performance"
      duration_seconds: 30
    
    - name: "scalability_test"
      description: "Test scaling with different worker counts"
      worker_counts: [1, 2, 4, 8]
    
    - name: "memory_stress_test"
      description: "Test behavior under memory pressure"
      memory_sizes_mb: [100, 500, 1000, 2000]
    
    - name: "cpu_stress_test"
      description: "Test behavior under CPU load"
      complexity_levels: [1000, 10000, 100000, 1000000]

# Output artifacts
artifacts:
  - name: "training_statistics"
    type: "json"
    path: "training_statistics.json"
    description: "Comprehensive training statistics and metrics"
  
  - name: "performance_graphs"
    type: "png"
    path: "performance_*.png"
    description: "Performance visualization graphs"
  
  - name: "system_profile"
    type: "json"
    path: "system_profile.json"
    description: "System resource profile and capabilities"
  
  - name: "worker_logs"
    type: "log"
    path: "worker_*.log"
    description: "Individual worker process logs"